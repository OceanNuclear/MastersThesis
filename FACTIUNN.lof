\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the structure of a typical neural network\relax }}{8}
\contentsline {figure}{\numberline {2}{\ignorespaces A ReLU function (a rectifying function)\relax }}{8}
\contentsline {figure}{\numberline {3}{\ignorespaces A cubic function approximated by a neural network\relax }}{9}
\contentsline {figure}{\numberline {4}{\ignorespaces The spectra predicted by a 2-hidden-layers neural network (with 16 nodes per layer) on a fully determined system.\relax }}{11}
\contentsline {figure}{\numberline {5}{\ignorespaces Data augmentation performed to create simulated spectra.\relax }}{11}
\contentsline {figure}{\numberline {6}{\ignorespaces The neural network's attempt at predicting a perturbed JET first wall spectrum.\relax }}{12}
\contentsline {figure}{\numberline {7}{\ignorespaces All fusion spectra used, obtained from \cite {ukaea_unfolding_support_report_march2017}\relax }}{12}
\contentsline {figure}{\numberline {8}{\ignorespaces Microscopic cross-section of each reaction\relax }}{13}
\contentsline {figure}{\numberline {9}{\ignorespaces By visualizing the loss values attained by each neural network using a heatmap, the neural network which perform the best can be identified. For this particular heatmap, the loss value of the neural network's prediction on the test dataset (a set of fusion spectra that it has never seen before) is visualized, with darker colours representing lower loss values (good performance), vice versa. At the top right hand corner, i.e. the neural network with the hyperparameters of (learning rate=0.01, number of nodes in each layer=[32,53,90,152,256]), a particularly loss value is obtained. Therefore this neural network is regarded as the neural network with the optimal hyperparameter, and further investigation into the predictions of this neural network is conducted to see if it is purely coincidental or not. \relax }}{14}
\contentsline {figure}{\numberline {10}{\ignorespaces \relax }}{14}
\contentsline {figure}{\numberline {11}{\ignorespaces While other neural networks struggles to keep a close match to the profile. This neural network is Even though the neural netwrok attempts to follow the general rise-and-fall of the true spectrum, it does not follow the true spectrum's gradient closely enough, leading to extreme discrepancies at some bins (e.g. above 14.1MeV).\relax }}{15}
\contentsline {figure}{\numberline {12}{\ignorespaces But it also sucks as an a priori generator, giving a loss value of 9.89936, which is only a marginal improvement on the 10.188233 stated in Figure\nobreakspace {}13\hbox {}\relax }}{15}
\contentsline {figure}{\numberline {13}{\ignorespaces Which is even worse than naive prior, i.e. using a flat a priori and thus giving no meaningful information to gravel before unfolding. In this case it achieves an average mean squared error of 10.188233\relax }}{16}
\contentsline {figure}{\numberline {14}{\ignorespaces loss value still average to 43.33816909790039\relax }}{17}
\contentsline {figure}{\numberline {15}{\ignorespaces the best one looks like this\relax }}{17}
