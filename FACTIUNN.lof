\contentsline {figure}{\numberline {1}{\ignorespaces structure of a typical neural network\relax }}{8}
\contentsline {figure}{\numberline {2}{\ignorespaces A ReLU function (a rectifying function). Function input=x-axis; function output=y-axis.\relax }}{8}
\contentsline {figure}{\numberline {3}{\ignorespaces A cubic function approximated by a neural network with 1 hidden layer containing 6 neurons.\relax }}{9}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$R$ is an alias for the ReLU function (see Figure\nobreakspace {}2\hbox {}), and $y$ is the output layer neuron's activation value, obtained by summing over the product of the activation of the $i^{th}$ neuron $s_i$ with the weight of its connection to the final layer, $w_i$.}}}{9}
\contentsline {figure}{\numberline {4}{\ignorespaces 16 node per layer, 2 layers\relax }}{11}
\contentsline {figure}{\numberline {5}{\ignorespaces On the left is an example of parametrisation performed on the JET spectra. These flux values are the total flux inside each energy bin, NOT divided by the lethargy span of each bin, so they are scaled up/down by larger/smaller bins. Despite the lack of smoothness after plotting these spectra, the Neural network was still able to identify and replicate the covariance between neighbouring channels. On the right is an example of ...[expansion needed], plotted according to their neutron flux per unit lethargy.\relax }}{11}
\contentsline {figure}{\numberline {6}{\ignorespaces The neural network excels at approximating a JET First Wall spectrum after being trained on variants of the same spectrum (only with slighly different peak heights), along with the perturbed spectra of 6 other fusion neutron sources.\relax }}{12}
\contentsline {figure}{\numberline {7}{\ignorespaces The neural networks were trained on 19 fusion neutron spectra, which were all rebinned into the Vitamin-J group structure.\relax }}{12}
\contentsline {figure}{\numberline {8}{\ignorespaces Microscopic cross-section values, obtained from [citation needed], were used as the response function for each of the reaction. No correction factors were applied as this is only a proof-of-concept experiment; applying correction factors that account for the number densities and foil volumes will only multiply the reaction rates by a fixed constant.\relax }}{13}
\contentsline {figure}{\numberline {9}{\ignorespaces By visualizing the loss values attained by each neural network using a heatmap, the neural network which perform the best can be identified. For this particular heatmap, the loss value of the neural network's prediction on the test dataset (a set of fusion spectra that it has never seen before) is visualized, with darker colours representing lower loss values (good performance), vice versa. At the top right hand corner, i.e. the neural network with the hyperparameters of (learning rate=0.01, number of nodes in each layer=[32,53,90,152,256]), a particularly loss value is obtained. Therefore this neural network is regarded as the neural network with the optimal hyperparameter, and further investigation into the predictions of this neural network is conducted to see if it is purely coincidental or not. \relax }}{13}
\contentsline {figure}{\numberline {10}{\ignorespaces \relax }}{14}
\contentsline {figure}{\numberline {11}{\ignorespaces While other neural networks struggles to keep a close match to the profile. This neural network is Even though the neural netwrok attempts to follow the general rise-and-fall of the true spectrum, it does not follow the true spectrum's gradient closely enough, leading to extreme discrepancies at some bins (e.g. above 14.1MeV).\relax }}{14}
\contentsline {figure}{\numberline {12}{\ignorespaces But it also sucks as an a priori generator, giving a loss value of 9.89936, which is only a marginal improvement on the 10.188233 stated in Figure\nobreakspace {}13\hbox {}\relax }}{15}
\contentsline {figure}{\numberline {13}{\ignorespaces Which is even worse than naive prior, i.e. using a flat a priori and thus giving no meaningful information to gravel before unfolding. In this case it achieves an average mean squared error of 10.188233\relax }}{16}
\contentsline {figure}{\numberline {14}{\ignorespaces loss value still average to 43.33816909790039\relax }}{17}
\contentsline {figure}{\numberline {15}{\ignorespaces the best one looks like this\relax }}{17}
